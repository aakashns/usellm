---
title: Quickstart
description: Add large language models to your React app with just a few lines of code.
---

`useLLM` is a React hook for integrating large language models like OpenAI's ChatGPT with just a few lines of code. [Try Demo](https://usellm.org/demo)

## Installation

Install the package from NPM:

```bash
npm install usellm@latest
```

## Usage

The libary offers the following functionality:

1. Use the `useLLM` hook in your react component (client-side)
2. Use `createLLMService` to create an API endpoint for the hook (server-side)
3. User `llmService.registerTemplate` to set up preconfigured prompts & options (server-side)

**NOTE**: This library is currently a wrapper over [OpenAI's chat completions API](https://platform.openai.com/docs/api-reference/chat/create). More language models and APIs will be added soon.

### Step 1 - `useLLM` hook

1. Import the `useLLM` hook from the `usellm` package:

```javascript
import useLLM from "usellm";
```

2. Initialize the hook with a Service URL inside a react component:

```javascript
const llm = useLLM({ serviceUrl: "https://usellm.org/api/llm" });
```

**NOTE**: The above service URL is for testing only, please don't use it in production. Check the next section to learn how you can create your own service URL.

3. Use the `llm.chat` method (possibly in an button click handler) to send a chat message :

```javascript
try {
  const { message } = await llm.chat({
    messages: [{ role: "user", content: "Who are you?" }],
  });
  console.log("Received message: ", message.content);
} catch (error) {
  console.error("Something went wrong!", error);
}
```

The `llm.chat` method also supports streaming the response token-by-token using the `onStream` callback:

```javascript
await llm.chat({
  messages: [{ role: "user", content: "Who are you?" }],
  stream: true,
  onStream: (message) => console.log(message),
});
```

The `messages` option passed to `llm.chat` must be an array of OpenAI messages, [as documented here](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages).

#### Example - `llm.chat`

Here's a complete working example that you can use as a starting point ([live demo](/demo1)):

```javascript
"use client";
import useLLM from "usellm";
import { useState } from "react";

export default function Demo() {
  const llm = useLLM({ serviceUrl: "https://usellm.org/api/llm" });
  const [result, setResult] = useState("");

  async function handleClick() {
    try {
      await llm.chat({
        messages: [{ role: "user", content: "What is a language model?" }],
        stream: true,
        onStream: ({ message }) => setResult(message.content),
      });
    } catch (error) {
      console.error("Something went wrong!", error);
    }
  }
  return (
    <div>
      <button onClick={handleClick}>Send</button>
      <div style={{ whiteSpace: "pre-wrap" }}>{result}</div>
    </div>
  );
}
```

It produces the following result:

<img
  src="https://github.com/usellm/usellm/assets/1560745/d4709dfa-9403-4845-9f76-2fa21667604a"
  alt="usellmdemo"
  width="320"
/>

#### Options - `useLLM`

Here is the type signature showing the full set of options supported by `llm.chat`:

```javascript
interface UseLLMChatOptions {
  messages?: OpenAIMessage[]; // message history for generating the next message
  stream?: boolean; // do you want to stream the response token by token?
  template?: string; // use a preconfigured template (see the `registerTemplate` section)
  inputs?: object; // inputs to be provided to the preconfigured template (see the `registerTemplate` section)
  onStream?: (result: {
    message: OpenAIMessage, // partial message containing tokens received so far
    isFirst: boolean, // is this the first token?
    isLast: boolean, // is this the last token?
  }) => void; // called every time a new token is received (only if stream is true)
}
```

The return value of `llm.chat` is a promise that resolves to an object containing the following fields:

```javascript
interface LLMChatResult {
  message: OpenAIMessage; // the final message received from OpenAI
}
```

`OpenAIMessage` has the following signature, [as documented here](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages):

```javascript
interface OpenAIMessage {
  content: string; // contains the actual text of the message
  role: string; // can be "system", "user", or "assistant"
  user?: string; // an optional user name/ID used by OpenAI for spam prevention
}
```

Apart from a service URL, you can also provide a custom `fetcher` function to `useLLM` for adding headers, cookies, etc. before invoking the service URL :

```javascript
const llm = useLLM({ serviceUrl: 'https://usellm.org/api/llm', fetcher: (url, options) => {...}});
```

The custom `fetcher` it should have the same signature at [default `fetch` function](https://developer.mozilla.org/en-US/docs/Web/API/fetch).

### Step 2 - Service URL with `createLLMService`

While you can use the example service URL https://usellm.org/api/llm for testing, you'll need to create your own service URL using the `createLLMService` function for most _real-world_ applications.

**Why?**: You'll need to provide an [API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) to use OpenAI APIs. This API key should NOT be sent to the browser i.e. it should only be used on the server-side. Additionally, you might also want to add authentication, rate limiting, model configuration options etc. on the server side.

Follow these steps to create your own service URL:

1. Create an LLM service:

```javascript
const llmService = createLLMService({
  openaiApiKey: process.env.OPENAI_API_KEY,
});
```

2. Store your OpenAI API key in an enviroment variable file (e.g. `.env.local` for [NextJS applications](https://nextjs.org/docs/app/building-your-application/configuring/environment-variables)):

```ini
OPENAI_API_KEY=xxxxxxxx
```

**Note**: Never hard code API keys in your code or check it into source control. Use an environment variable instead. You'll also need to supply this environment variable to your production environment.

3. Use the `llmService.handle` method in a server-side API route:

```javascript
/* pages/api/llm.js */

export default async function handler(req, res) {
  const result = await llmService.handle({ body: req.body, request: req });
  res.send(result);
}
```

**Note**: The above example creates a [NextJS API route](https://nextjs.org/docs/pages/building-your-application/routing/api-routes) using the [pages router](https://nextjs.org/docs/pages/building-your-application/routing).

4. Provide the API URL/path in the `useLLM` hook in your React component:

```javascript
const llm = useLLM({ serviceUrl: "/api/llm" });
```

**NOTE**: To stream messages in a NextJS application, you'll need to use the [`edge` runtime](https://nextjs.org/docs/app/building-your-application/rendering/edge-and-nodejs-runtimes).

#### Example - `createLLMService` with Next.js App Router

If you're using Next.js with the [`app` router](https://nextjs.org/docs/app), here's an example [edge route handler](https://nextjs.org/docs/app/building-your-application/routing/router-handlers#edge-and-nodejs-runtimes) you can use as a starting point:

```javascript
/* app/api/llm.ts */

import { createLLMService } from "usellm";

export const runtime = "edge";

const llmService = createLLMService({
  openaiApiKey: process.env.OPENAI_API_KEY,
});

export async function POST(request: Request) {
  const body = await request.json();

  try {
    const data = await llmService.handle({ body, request });
    return new Response(data, { status: 200 });
  } catch (error) {
    return new Response((error as Error).message, { status: 400 });
  }
}

```

#### Example - `createLLMService` with Next.js Pages Router

If you're using Next.js with the [`pages` router](https://nextjs.org/docs/pages/building-your-application) for routing, here's an example [edge API route](https://nextjs.org/docs/pages/building-your-application/routing/api-routes#edge-api-routes) you can use as a starting point:

```javascript
/* pages/api/llm.ts */

import { createLLMService } from "usellm";

export const config = {
  runtime: 'edge',
};

const llmService = createLLMService({
  openaiApiKey: process.env.OPENAI_API_KEY,
});

export default async function handler(request: Request) {
  const body = await request.json();

  try {
    const data = await llmService.handle({ body, request });
    return new Response(data, { status: 200 });
  } catch (error) {
    return new Response((error as Error).message, { status: 400 });
  }
}
```

**Tip**: You can use Upstash for rate limiting: https://upstash.com/blog/upstash-ratelimit

#### Options - `createLLMService`

Here are the full set of options you can provide to `createLLMService`:

```javascript
export interface CreateLLMServiceOptions {
  openaiApiKey?: string; // your OpenAI API Key (prefer passing this via an environment variable)
  fetcher?: typeof fetch; // provide a custom fetcher
  templates?: { [id: string]: LLMServiceTemplate }; // provide custom templates (see next section)
  debug?: boolean; // logs the JSON body sent to OpenAI
  isAllowed?: (body: LLMServiceBody) => boolean | Promise<boolean>; // For checking authentication, rate limiting etc.
}
```

`llmService.handle` accepts the following options:

```javascript
export interface LLMServiceHandleOptions {
  body: object; // the body of the request
  request?: Request; // the request itself (for authentication, rate limiting etc.)
}
```

Here are the default options passed to the OpenAI API along with the messages received in the request body (see the next section for customization):

```javascript
const defaultTemplate = {
  model: "gpt-3.5-turbo",
  max_tokens: 200,
  temperature: 0.8,
};
```

### Step 3 - Prompt & Options with `registerTemplate`

To generate useful responses from large language models like GPT-3.5/4, you might need to customize the [options passed to the API](https://platform.openai.com/docs/api-reference/chat/create) (e.g. `model`, `max_tokens`, `temperature`) and create default prompts that provide instructions to the model to generate the desired results.

To achieve this, you can create & register templates on the server using the `llmService.registerTemplate` method, and invoke registered templates in your React components using `llm.chat`.

1. Register a template on the server using the `registerTemplate` method :

```javascript
llmService.registerTemplate({
  id: "tutorial-generator",
  systemPrompt:
    "You job is to create a short tutorial on a given topic. Use simple words, avoid jargon. Start with an introduction, then provide a few points of explanation, and end with a conclusion",
  userPrompt: "Tell me about {{topic}}",
  model: "gpt-4",
  temperature: 0.8,
});
```

Note that you can provide two prompts within a template:

- `systemPrompt`: This is a system-level message that guides the behavior of the model throught the conversation. It is sent as `{"role": "system", "content": "..."}`.

- `userPrompt`: This is simply as the first message in the conversation, and is used to generate the first response. It is sent as `{"role": "user", "content": "..."}`.

Each prompt can contain variables e.g. `{{topic}}` whose values can be sent from the client (browser) using `llm.chat`.

2. Provide the template id and inputs in your React component while calling `llm.chat`:

```javascript
const { message } = await llm.chat({
  template: "tutorial-generator",
  inputs: { topic: "Machine Learning" },
});
```

If a `messages` argument is also provided to `llm.chat`, the provided messages are appended after the initial system and user prompts. Provide a list of `messages` to continue the conversation.

Check out this course on prompt engineering to craft effective prompts: https://learnprompting.org/docs/intro

#### Example - Template

Here's a complete example of using templates in a NextJS [app router](https://nextjs.org/docs/app) project ([live demo](https://usellm.org/demo2)):

```javascript
/* app/api/llm.tsx */

import { createLLMService } from "usellm";

export const runtime = "edge";

const llmService = createLLMService({
  openaiApiKey: process.env.OPENAI_API_KEY,
});

llmService.registerTemplate({
  id: "tutorial-generator",
  systemPrompt:
    "You job is to create a short tutorial on a given topic. Use simple words, avoid jargon. Start with an introduction, then provide a few points of explanation, and end with a conclusion",
  userPrompt: "Topic: {{topic}}",
  max_tokens: 1000,
  model: "gpt-4",
  temperature: 0.7,
});

export async function POST(request: Request) {
  const body = await request.json();
  try {
    const data = await llmService.handle({ body, request });
    return new Response(data, { status: 200 });
  } catch (error) {
    return new Response((error as Error).message, { status: 400 });
  }
}

```

```javascript
/* app/page.tsx */
"use client";
import { useState } from "react";
import useLLM from "usellm";

export default function HomePage() {
  const llm = useLLM({ serviceUrl: "/api/llm" });
  const [topic, setTopic] = useState("");
  const [result, setResult] = useState("");

  async function handleClick() {
    await llm.chat({
      template: "tutorial-generator",
      inputs: { topic },
      stream: true,
      onStream: (message) => setResult(message.content),
    });
  }

  return (
    <div>
      <input
        type="text"
        placeholder="Enter topic"
        value={topic}
        onChange={(e) => setTopic(e.target.value)}
      />
      <button onClick={handleClick}>Send</button>
      <div style={{ whiteSpace: "pre-wrap" }}>{result}</div>
    </div>
  );
}
```

This produces the following result:

<img
  src="https://github.com/usellm/usellm/assets/1560745/3c75c050-692a-4d7f-8620-545b32b626da"
  width="420"
  alt="template demo"
/>

#### Options - `registerTemplate`

Here are all the options supported for templates:

```javascript
interface LLMServiceTemplate {
  id: string; // globally unique identifier
  systemPrompt?: string; // system prompt to guide the model (can contain variables)
  userPrompt?: string; // user prompt to generate the first response (can contain variables)
  model?: string; // model to use (e.g. gpt-3.5-turbo)
  temperature?: number;
  top_p?: number;
  n?: number;
  max_tokens?: number;
  presence_penalty?: number;
  frequency_penalty?: number;
  logit_bias?: number;
}
```

Check the [OpenAI API docs](https://platform.openai.com/docs/api-reference/chat/create) for information about each option and view the [source code](https://github.com/usellm/usellm/blob/main/packages/usellm/src/createLLMService.ts) for more details.

## Contributing

The library is under active development. Please open an issue to report bugs and open a pull request to contribute new features.
